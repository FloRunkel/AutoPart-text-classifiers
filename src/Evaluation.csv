Model Description;F1-Score Micro;F1-Score Macro;MCC
Zero-Shot mit Modell: 'deepset/gbert-large';0.41960784313725485;0.3290867551208073;0.10779514510174475
Zero-Shot mit Modell: 'xlm-roberta-large';0.2823529411764706;0.22274805615192717;-0.021929301239663745
Few-Shot mit Modell: 'deepset/gbert-large', Lernrate: '1e-05'und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten;0.6745098039215687;0.20140515222482436;0.0
Few-Shot mit Modell: 'deepset/gbert-large', Lernrate: '2e-05'und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten;0.6745098039215687;0.21936903888481293;0.06847006977103823
Few-Shot mit Modell: 'deepset/gbert-large', Lernrate: '3e-05'und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten;0.6745098039215687;0.20140515222482436;0.0
Few-Shot mit Modell: 'deepset/gbert-large', Lernrate: '0.0004'und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten;0.6745098039215687;0.20140515222482436;0.0
Few-Shot mit Modell: 'deepset/gbert-large', Lernrate: '4e-05'und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten;0.6745098039215687;0.20140515222482436;0.0
Few-Shot mit Modell: 'deepset/gbert-large', Lernrate: '4e-06'und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten;0.21176470588235294;0.16735419540950708;0.006820661198368158
Few-Shot mit Modell: 'xlm-roberta-large', Lernrate: '1e-05'und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten;0.3058823529411765;0.19367654628051367;-0.025623114221792618
Few-Shot mit Modell: 'xlm-roberta-large', Lernrate: '2e-05'und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten;0.30196078431372547;0.1840059683378325;-0.06785629369924294
Few-Shot mit Modell: 'xlm-roberta-large', Lernrate: '3e-05'und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten;0.5215686274509804;0.20281316310145098;-0.01174468624425465
Few-Shot mit Modell: 'xlm-roberta-large', Lernrate: '0.0004'und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten;0.6078431372549019;0.1890243902439024;0.0
Few-Shot mit Modell: 'xlm-roberta-large', Lernrate: '4e-05'und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten;0.5607843137254902;0.19455128205128205;-0.02218905180432155
Few-Shot mit Modell: 'xlm-roberta-large', Lernrate: '4e-06'und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten;0.3137254901960784;0.19827008829094506;-0.044643919095301014
Multi-Class Textclassfikation mit Modell: BERT , Lernrate: '1e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten.;0.8410852713178295;0.7473630126002266;0.6724012977475159
Multi-Class Textclassfikation mit Modell: BERT , Lernrate: '2e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten.;0.8178294573643411;0.705777665317139;0.6326990511003426
Multi-Class Textclassfikation mit Modell: BERT , Lernrate: '3e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten.;0.8294573643410853;0.7878419452887538;0.6980857706496787
Multi-Class Textclassfikation mit Modell: BERT , Lernrate: '0.0004' und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten.;0.686046511627907;0.20344827586206898;0.0
Multi-Class Textclassfikation mit Modell: BERT , Lernrate: '4e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten.;0.7868217054263565;0.725242914979757;0.6301226314753903
Multi-Class Textclassfikation mit Modell: BERT , Lernrate: '4e-06' und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten.;0.755813953488372;0.3748115577889447;0.4154591399102715
Multi-Class Textclassfikation mit Modell: Modell: RoBERTa , Lernrate: '1e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten.;0.7945736434108527;0.654906656841071;0.5812130784626404
Multi-Class Textclassfikation mit Modell: Modell: RoBERTa , Lernrate: '2e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten.;0.8062015503875969;0.7280839979418575;0.6362870886581631
Multi-Class Textclassfikation mit Modell: Modell: RoBERTa , Lernrate: '3e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten.;0.7945736434108527;0.7084724845880025;0.6050997718580705
Multi-Class Textclassfikation mit Modell: Modell: RoBERTa , Lernrate: '0.0004' und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten.;0.686046511627907;0.20344827586206898;0.0
Multi-Class Textclassfikation mit Modell: Modell: RoBERTa , Lernrate: '4e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten.;0.8062015503875969;0.7377279431482971;0.6360084698987443
Multi-Class Textclassfikation mit Modell: Modell: RoBERTa , Lernrate: '4e-06' und einer Epochen-Anzahl von '5' und der Verwendung von 50% der verfuegbaren Daten.;0.755813953488372;0.3779166666666667;0.4133757137408598

Zero-Shot mit Modell: 'deepset/gbert-large';0.3588235294117647;0.2742319663494027;0.03706562219141356
Zero-Shot mit Modell: 'xlm-roberta-large';0.2735294117647059;0.2279044070701523;0.011898008775321121
Few-Shot mit Modell: 'deepset/gbert-large', einer Lernrate von '1e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.6411764705882353;0.19569120287253144;-0.02151704750480607
Few-Shot mit Modell: 'deepset/gbert-large', einer Lernrate von '2e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.5794117647058824;0.18342644320297952;-0.028807705782990686
Few-Shot mit Modell: 'deepset/gbert-large', einer Lernrate von '3e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.5823529411764706;0.18401486988847582;0.0
Few-Shot mit Modell: 'deepset/gbert-large', einer Lernrate von '0.0004' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.8000000000000002;0.7098197897547138;0.6057671306455921
Few-Shot mit Modell: 'deepset/gbert-large', einer Lernrate von '4e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.65;0.196969696969697;0.0
Few-Shot mit Modell: 'deepset/gbert-large', einer Lernrate von '4e-06' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.5705882352941176;0.21528261919197744;-0.03145287765452257
Few-Shot mit Modell: 'xlm-roberta-large', einer Lernrate von '1e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.35;0.21787418008348242;-0.03676552699772625
Few-Shot mit Modell: 'xlm-roberta-large', einer Lernrate von '2e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.538235294117647;0.20660388864408913;-0.0017408187359763756
Few-Shot mit Modell: 'xlm-roberta-large', einer Lernrate von '3e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.55;0.17775665399239543;-0.08564585698154424
Few-Shot mit Modell: 'xlm-roberta-large', einer Lernrate von '0.0004' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.6588235294117647;0.24835669596895538;0.1358820007036754
Few-Shot mit Modell: 'xlm-roberta-large', einer Lernrate von '4e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.65;0.196969696969697;0.0
Few-Shot mit Modell: 'xlm-roberta-large', einer Lernrate von '4e-06' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.2529411764705882;0.1704271927461147;-0.05277801870885905
Multi-Class Textclassfikation mit Modell: BERT , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.7507246376811595;0.5302980132450331;0.5348509476508886
Multi-Class Textclassfikation mit Modell: BERT , einer Lernrate von '2e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.8028985507246377;0.7586731797575069;0.6549692786625697
Multi-Class Textclassfikation mit Modell: BERT , einer Lernrate von '3e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.8028985507246377;0.7648209348319961;0.6600202376752069
Multi-Class Textclassfikation mit Modell: BERT , einer Lernrate von '0.0004' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.6028985507246377;0.18806509945750455;0.0
Multi-Class Textclassfikation mit Modell: BERT , einer Lernrate von '4e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.8144927536231884;0.7622728719757077;0.6778597412643088
Multi-Class Textclassfikation mit Modell: BERT , einer Lernrate von '4e-06' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.7246376811594203;0.39937804453723036;0.47116162519439014
Multi-Class Textclassfikation mit Modell: RoBERTa , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.8231884057971014;0.7869956088168185;0.6938056541037467
Multi-Class Textclassfikation mit Modell: RoBERTa , einer Lernrate von '2e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.8260869565217391;0.7902409739184412;0.7007314492906007
Multi-Class Textclassfikation mit Modell: RoBERTa , einer Lernrate von '3e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.8057971014492754;0.7680626872058194;0.6673806162695506
Multi-Class Textclassfikation mit Modell: RoBERTa , einer Lernrate von '0.0004' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.6028985507246377;0.18806509945750455;0.0
Multi-Class Textclassfikation mit Modell: RoBERTa , einer Lernrate von '4e-05' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.7942028985507247;0.7015728967800126;0.6307021290405918
Multi-Class Textclassfikation mit Modell: RoBERTa , einer Lernrate von '4e-06' und einer Epochen-Anzahl von '5' und der Verwendung von 100% der Verfuegbaren Daten.;0.8231884057971014;0.7818222891566264;0.6929774275491878

Zero-Shot Modell: RoBERTa, mit 70% der Daten;0.27647058823529413;0.22069156363072873;0.014303290781641562
Zero-Shot Modell:BERT, mit 70% der Daten ;0.38529411764705884;0.31815869315869316;0.08323606181902463
Few-Shot mit Modell: 'deepset/gbert-large', einer Lernrate von '1e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.6033755274261603;0.1881578947368421;0.0
Few-Shot mit Modell: 'deepset/gbert-large', einer Lernrate von '2e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.6371308016877637;0.19458762886597938;0.0
Few-Shot mit Modell: 'deepset/gbert-large', einer Lernrate von '3e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.6371308016877637;0.19458762886597938;0.0
Few-Shot mit Modell: 'deepset/gbert-large', einer Lernrate von '0.0004' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.7805907172995781;0.750986688872513;0.6130820559179244
Few-Shot mit Modell: 'deepset/gbert-large', einer Lernrate von '4e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.6033755274261603;0.1881578947368421;0.0
Few-Shot mit Modell: 'deepset/gbert-large', einer Lernrate von '4e-06' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.5443037974683544;0.3409267021270526;0.11200669804247755
Few-Shot mit Modell: 'xlm-roberta-large', einer Lernrate von '1e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der verfuegbaren Daten.;0.5485232067510548;0.2416339573268921;0.0035841548821106315
Few-Shot mit Modell: 'xlm-roberta-large', einer Lernrate von '2e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der verfuegbaren Daten.;0.5907172995780591;0.24643091873153888;-0.026178972813674484
Few-Shot mit Modell: 'xlm-roberta-large', einer Lernrate von '3e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der verfuegbaren Daten.;0.6540084388185654;0.19770408163265307;-0.025009659891221076
Few-Shot mit Modell: 'xlm-roberta-large', einer Lernrate von '0.0004' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der verfuegbaren Daten.;0.7341772151898734;0.37944822071171536;0.41151662532138433
Few-Shot mit Modell: 'xlm-roberta-large', einer Lernrate von '4e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der verfuegbaren Daten.;0.6582278481012658;0.1984732824427481;0.0
Few-Shot mit Modell: 'xlm-roberta-large', einer Lernrate von '4e-06' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der verfuegbaren Daten.;0.48945147679324896;0.2230564155486756;0.001456864150059407
Multi-Class Textclassfikation mit Modell: BERT , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.7883817427385892;0.7370632644169425;0.6308558779699175
Multi-Class Textclassfikation mit Modell: BERT , einer Lernrate von '2e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.7800829875518672;0.7432933059575966;0.6353182425892921
Multi-Class Textclassfikation mit Modell: BERT , einer Lernrate von '3e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.7634854771784232;0.7431024454391422;0.6238308036340192
Multi-Class Textclassfikation mit Modell: BERT , einer Lernrate von '0.0004' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.6182572614107884;0.19102564102564104;0.0
Multi-Class Textclassfikation mit Modell: BERT , einer Lernrate von '4e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.8008298755186722;0.7327955791671735;0.6380540634790098
Multi-Class Textclassfikation mit Modell: BERT , einer Lernrate von '4e-06' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.8008298755186722;0.7494080469309212;0.6526803097500147
Multi-Class Textclassfikation mit Modell: RoBERTa , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.8257261410788381;0.7986238242898717;0.7006189092252674
Multi-Class Textclassfikation mit Modell: RoBERTa , einer Lernrate von '2e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.7883817427385892;0.7660588440292448;0.6663075897005496
Multi-Class Textclassfikation mit Modell: RoBERTa , einer Lernrate von '3e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.7676348547717842;0.7311318831954272;0.611728408318792
Multi-Class Textclassfikation mit Modell: RoBERTa , einer Lernrate von '0.0004' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.6182572614107884;0.19102564102564104;0.0
Multi-Class Textclassfikation mit Modell: RoBERTa , einer Lernrate von '4e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.8091286307053942;0.7572392951057734;0.6598443939989389
Multi-Class Textclassfikation mit Modell: RoBERTa , einer Lernrate von '4e-06' und einer Epochen-Anzahl von '10' und der Verwendung von 70% der Verfuegbaren Daten.;0.8008298755186722;0.732975698088572;0.6512274929260846

Few-Shot mit Modell: 'deepset/gbert-large', Lernrate: '1e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der Verfuegbaren Daten.;0.615686274509804;0.19053398058252427;0.0
Few-Shot mit Modell: 'deepset/gbert-large', Lernrate: '2e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der Verfuegbaren Daten.;0.615686274509804;0.19053398058252427;0.0
Few-Shot mit Modell: 'deepset/gbert-large', Lernrate: '3e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der Verfuegbaren Daten.;0.615686274509804;0.19053398058252427;0.0
Few-Shot mit Modell: 'deepset/gbert-large', Lernrate: '0.0004' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der Verfuegbaren Daten.;0.8117647058823529;0.7268657811006056;0.6474756675235563
Few-Shot mit Modell: 'deepset/gbert-large', Lernrate: '4e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der Verfuegbaren Daten.;0.615686274509804;0.19053398058252427;0.0
Few-Shot mit Modell: 'deepset/gbert-large', Lernrate: '4e-06' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der Verfuegbaren Daten.;0.6019607843137255;0.23912491017754176;0.04530604243445874
Few-Shot mit Modell: 'xlm-roberta-large', Lernrate: '1e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der Verfuegbaren Daten.;0.6039215686274509;0.1894218942189422;-0.054252136625002154
Few-Shot mit Modell: 'xlm-roberta-large', Lernrate: '2e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der Verfuegbaren Daten.;0.6352941176470588;0.1944777911164466;-0.032013934631121
Few-Shot mit Modell: 'xlm-roberta-large', Lernrate: '3e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der Verfuegbaren Daten.;0.6450980392156863;0.1960667461263409;0.0
Few-Shot mit Modell: 'xlm-roberta-large', Lernrate: '0.0004' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der Verfuegbaren Daten.;0.6450980392156863;0.20656107140557595;0.04041025183482664
Few-Shot mit Modell: 'xlm-roberta-large', Lernrate: '4e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der Verfuegbaren Daten.;0.6431372549019608;0.1957040572792363;-0.01804700533611502
Few-Shot mit Modell: 'xlm-roberta-large', Lernrate: '4e-06' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der Verfuegbaren Daten.;0.2529411764705882;0.1921959308519779;-0.06241455806319019
Multi-Class Textclassfikation mit Modell: BERT , Lernrate: '1e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der verfuegbaren Daten.;0.8201160541586073;0.7728243803170824;0.6890495150587621
Multi-Class Textclassfikation mit Modell: BERT , Lernrate: '2e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der verfuegbaren Daten.;0.816247582205029;0.7689303337661119;0.6744250295641205
Multi-Class Textclassfikation mit Modell: BERT , Lernrate: '3e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der verfuegbaren Daten.;0.8201160541586073;0.775135761906331;0.6910133653306552
Multi-Class Textclassfikation mit Modell: BERT , Lernrate: '0.0004' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der verfuegbaren Daten.;0.6286266924564797;0.19299287410926366;0.0
Multi-Class Textclassfikation mit Modell: BERT , Lernrate: '4e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der verfuegbaren Daten.;0.8123791102514507;0.772076371803626;0.6683061182022626
Multi-Class Textclassfikation mit Modell: BERT , Lernrate: '4e-06' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der verfuegbaren Daten.;0.8201160541586073;0.7733290047608539;0.6807904993700798
Multi-Class Textclassfikation mit Modell: Modell: RoBERTa , Lernrate: '1e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der verfuegbaren Daten.;0.8278529980657641;0.7926803804297453;0.698339557480403
Multi-Class Textclassfikation mit Modell: Modell: RoBERTa , Lernrate: '2e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der verfuegbaren Daten.;0.8104448742746615;0.7673440692892013;0.6602186422720118
Multi-Class Textclassfikation mit Modell: Modell: RoBERTa , Lernrate: '3e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der verfuegbaren Daten.;0.8181818181818182;0.780188627477928;0.6851597263714384
Multi-Class Textclassfikation mit Modell: Modell: RoBERTa , Lernrate: '0.0004' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der verfuegbaren Daten.;0.6286266924564797;0.19299287410926366;0.0
Multi-Class Textclassfikation mit Modell: Modell: RoBERTa , Lernrate: '4e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der verfuegbaren Daten.;0.816247582205029;0.7706706420294125;0.675437553051421
Multi-Class Textclassfikation mit Modell: Modell: RoBERTa , Lernrate: '4e-06' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der verfuegbaren Daten.;0.8143133462282399;0.7565193393067128;0.6707742918756568

Finden des Finalen Modells
Multi-Class Textclassfikation mit Modell: RoBERTa , einer Lernrate von '4e-06' und einer Epochen-Anzahl von '20' und der Verwendung von 100% der verfuegbaren Daten.;0.8492753623188406;0.8092933096366712;0.7241328156634111
Multi-Label Classic Textclassfikation mit Modell: RoBERTa , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '10' und der Verwendung von 100% der verfuegbaren Daten.;0.8459657701711492;0.8155686202389604;kein support fuer MultiLabel

Multi-Label Classic Textclassfikation mit Modell: RoBERTa , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '20' und der Verwendung von 100% der verfuegbaren Daten.;0.8487690504103166;0.8383483385359828;kein support fuer MultiLabel Multi-Label Classic Textclassfikation mit Modell: RoBERTa , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '30' und der Verwendung von 100% der verfuegbaren Daten.;0.8183962264150944;0.8169548759771079;kein support fuer MultiLabel

Multi-Label Classic Textclassfikation mit Modell: RoBERTa , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '18' und der Verwendung von 100% der verfuegbaren Daten.;0.8390129259694478;0.8337146368067158;kein support fuer MultiLabel
Multi-Label Classic Textclassfikation mit Modell: RoBERTa , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '23' und der Verwendung von 100% der verfuegbaren Daten.;0.830188679245283;0.8082370471245152;kein support fuer MultiLabel
Multi-Label Classic Textclassfikation mit Modell: RoBERTa , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '18' und der Verwendung von 100% der verfuegbaren Daten.;0.830188679245283;0.8183577932771482;kein support fuer MultiLabel
 Multi-Label Classic Textclassfikation mit Modell: RoBERTa , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '18' und der Verwendung von 100% der verfuegbaren Daten.;0.808411214953271;0.8022589513176508;kein support fuer MultiLabel
 Multi-Label Classic Textclassfikation mit Modell: RoBERTa , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '13' und der Verwendung von 100% der verfuegbaren Daten.;0.8179669030732861;0.7990440624614936;kein support fuer MultiLabel
 Multi-Label Classic Textclassfikation mit Modell: RoBERTa , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '20' und der Verwendung von 100% der verfuegbaren Daten.;0.8028169014084506;0.7983296580913015;kein support fuer MultiLabel
 Multi-Label Classic Textclassfikation mit Modell: RoBERTa , einer Lernrate von '1e-05' und einer Epochen-Anzahl von '20' und der Verwendung von 100% der verfuegbaren Daten.;0.8827751196172248;0.8736697222270862;kein support fuer MultiLabel
